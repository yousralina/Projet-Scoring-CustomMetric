{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing part\n",
    "Description of the project"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classic.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data Viz.\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Preprocessing.\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import PolynomialFeatures, MinMaxScaler\n",
    "\n",
    "# Import custom functions.\n",
    "from Utils.utils_preprocessing import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format & option.\n",
    "sns.set(rc={'figure.figsize':(16,9)})\n",
    "pd.options.display.max_columns = 100\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
    "\n",
    "# Style use.\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "# Filter warnings.\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data.\n",
    "df_app_train = pd.read_csv(\"Data/application_train.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Cleaning and Preprocessing\n",
    "\n",
    "- Delete observation with not enough information\n",
    "- Filter outliers\n",
    "- Delete categorical features with not enough information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsampling of the majority class.\n",
    "df_app_train['Nbr_nan'] = df_app_train[df_app_train['TARGET']==0].isna().sum(axis=1)\n",
    "df_app_train = df_app_train[(df_app_train.Nbr_nan < 48) | (df_app_train.Nbr_nan != np.nan)]\n",
    "del df_app_train['Nbr_nan'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace outliers by NaN\n",
    "df_app_train['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your selected dataframe has 122 columns.\n",
      "There are 68 columns that have missing values.\n"
     ]
    }
   ],
   "source": [
    "# Check missing values on features.\n",
    "df_missing_values = missing_values_table(df_app_train)\n",
    "\n",
    "# Check on categorical features.\n",
    "# Delete 1 feature with too many missing values.\n",
    "df_missing_values[df_missing_values.Feature_type == object]\n",
    "del df_app_train[\"FONDKAPREMONT_MODE\"]\n",
    "\n",
    "# Check on numerical features.\n",
    "# Delete 1 feature with too many missing values.\n",
    "df_missing_values[df_missing_values.Feature_type == 'float64']\n",
    "del df_app_train[\"OWN_CAR_AGE\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Preprocessing for numerical and categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split in training and testing set.\n",
    "X, y = df_app_train.iloc[:, df_app_train.columns != \"TARGET\"], df_app_train.TARGET\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# Store id_clients.\n",
    "id_clients_train = X_train[\"SK_ID_CURR\"]\n",
    "id_clients_test = X_test[\"SK_ID_CURR\"]\n",
    "\n",
    "# Deleted id_clients before preprocessing.\n",
    "del X_train[\"SK_ID_CURR\"]\n",
    "del X_test[\"SK_ID_CURR\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputation on continuous features.\n",
    "X_train_num, X_test_num = X_train.select_dtypes(exclude=[\"object\"]), X_test.select_dtypes(exclude=[\"object\"])\n",
    "\n",
    "# Imputation.\n",
    "impute = IterativeImputer(n_nearest_features=15, imputation_order='ascending', random_state=42)\n",
    "X_train_num = pd.DataFrame(impute.fit_transform(X_train_num), columns=X_train_num.columns)\n",
    "X_test_num = pd.DataFrame(impute.transform(X_test_num), columns=X_test_num.columns)\n",
    "\n",
    "# Rescaling.\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_train_num = pd.DataFrame(scaler.fit_transform(X_train_num), columns=X_train_num.columns)\n",
    "X_test_num = pd.DataFrame(scaler.fit_transform(X_test_num), columns=X_train_num.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding and imputation on categorical features.\n",
    "X_train_categ, X_test_categ = impute_cat_feature(X_train, X_test, y_train)\n",
    "X_train_categ.dropna(inplace=True, axis=1)\n",
    "X_test_categ.dropna(inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset indexes.\n",
    "X_train_num.reset_index(inplace=True, drop=True)\n",
    "X_train_categ.reset_index(inplace=True, drop=True)\n",
    "X_test_num.reset_index(inplace=True, drop=True)\n",
    "X_test_categ.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Concat.\n",
    "X_train = pd.concat([X_train_num, X_train_categ], axis=1)\n",
    "X_test = pd.concat([X_test_num, X_test_categ], axis=1)\n",
    "\n",
    "# Reindex with id_clients\n",
    "X_train.SK_ID_CURR = id_clients_train\n",
    "X_test.SK_ID_CURR = id_clients_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data.\n",
    "X_train.to_csv(\"Data/X_train.csv\", index=False)\n",
    "X_test.to_csv(\"Data/X_test.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AutoML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "787fa561245cf4010031a667de2da7676de0c02020ea5bef7d705f157d1b95f3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
